---
title: "Case 2 Part 3"
author: "Sonia Xu, Grant Goettel, Ian Hua"
date: "October 22, 2017"
output: html_document
---

```{r echo = F, warning = F, include = F}
knitr::opts_chunk$set(fig.width=5, fig.height=3) 
library(dplyr)
library(magrittr)
library(knitr)
library(ggplot2)
```


```{r include = F}
kelly <- read.table("kellydat.txt", header = T)
kelly <- kelly %>% mutate(count_sn = sn1 + sn2*2 + sn3*3 + all4*4) %>% select(-c(sn1, sn2, sn3, all4))
```

```{r echo = F}
#functions
kern_distribution <- function(x,knots = 4) {
  ### s: #controls how wide the kernels are, s should be half the distance between two knots 
  ### tau: decides where the breaks are based on the number of knots
  ###knots: number of splits/peaks in the data
  ##x and y are the data
  
  tau <- seq(min(x), max(x), length.out = knots)
  s <- diff(tau)[1]/2
  X <- matrix(0, nrow=length(x),ncol=knots)
  for(i in 1:knots) {
    X[,i] <- x * dnorm(x, tau[i], s)
  }
  return(X)
}
```

<!-- BACKGROUND: Study of time to critical neurological assessment for patients with stroke-like symptoms who are admitted to the emergency room.  A possible predictor is number of major stroke symptoms reported, ranging from 0 to 4.  Treatment for acute stroke includes thrombolytic therapy, which can potentially improve neurological functioning for ischemic stroke patients if administered soon after symptom onset (within 3 hours).  Since treating patients quickly is critically important for their long-term prognosis, minimizing the times from symptom onset to emergency room (ED) arrival, from ED arrival to diagnosis, and from diagnosis to treatment is of paramount concern.
INTEREST: Factors predictive of the time to critical neurological assessment following admission to the ED for n=335 patients with mild to moderate motor impairment.  The goal of the analysis is to perform inferences on the impact of clinical presentation, gender and race on time to neurological assessment.  Clinical presentation is measured as a count of reported major stroke symptoms, including headache, loss of motor skills or weakness, trouble talking or understanding, and vision problems.
-->
#Introduction
The following data is from a study of time to critical neurological assessment for patients with stroke-like symptoms who were admitted to the emergency room. The purpose of the analysis is to perform inferences on the impact of clinical presentation (reported number of makor stroke symptoms), gender, and race on time to neurological asessment. This paper fits a model that identifies the differences in wait time to neurological assessment based on these features of interest. A final model of kernel regression with a response of fail, continuous feature of nctdel estimated via kernel regression, and binary features of black, hispanic, male, number of symptoms.

#Data set
The data set contained information about the amount of time elapsed from arrival at the ER to the assessment, and whether or not the patient received a CT scan. It also contained data about whether each patient was male or female, whether they were black or hispanic or not, and the amount of major symptoms of a stroke they had upon arrival (out of 4 main symptoms). Rather than have four separate indicator variables showing whether they had 1, 2, 3, or 4 symptoms, we created a single numerical variable containing the amount of symptoms.

#Methodology
To assess the impact of clinical presentation, gender and race on time to neurological assessment, multiple models and analyses were explored to find the best model that fit the data. After exploring Cox Proportional Hazards, Kaplan-Meier Estimate, Random Forest, and Kernel Regression, the Kernel Regression Model seemed to perform the best despite its flaws. We chose 10 bins based on quantiles to account for the uneven frequencies of the data. 

#EDA

##Racial Bias
###Black vs. Non-Black
There could potentially be bias here, but more analysis should be done. Non-blacks appear to have shorter wait times, but some non-blacks have extremely long wait times.

```{r echo = F}

ggplot(kelly, aes(nctdel, group = factor(black), fill = factor(black,labels=c("no", "yes"))),alpha=0.5, adjust=2) + geom_density() + theme_bw() + xlab("Wait Time") + ylab("Frequency of Wait Time") + labs(fill = "Black?") + ggtitle("Differences in Wait Time if Black")
```

###Hispanic vs. Non-Hispanic
There could potentially be bias here, but the lack of sample size for Hispanics appears to be an issue.
```{r echo = F}
ggplot(kelly, aes(nctdel, group = factor(hisp), fill = factor(hisp,labels=c("no", "yes"))),alpha=0.5, adjust=2) + geom_density() + theme_bw() + xlab("Wait Time") + ylab("Frequency of Wait Time") + labs(fill = "Hispanic?") + ggtitle("Differences in Wait Time if Hispanic")
```


##Gender Bias
There could potentially be bias here, but more analysis should be done. Females appear to have shorter wait times.

```{r echo = F}
ggplot(kelly, aes(nctdel, group = factor(male), fill = factor(male,labels=c("no", "yes"))),alpha=0.5, adjust=2) + geom_density() + theme_bw() + xlab("Wait Time") + ylab("Frequency of Wait Time") + labs(fill = "Male?") + ggtitle("Differences in Wait Time if Male")
```

#Final Model

We decided to create a Kernel Regression model with failure as the response to explore the robustness of this new model.

##Kernel Regression with 10 Bins
10 bins were calculated to fit the kernels. The bins are unevenly spaced because the data has a higher concentration of points for the feature nctdel between 0 and 2, even though its range is (0,26.25). We chose 10 bins because we wanted to be careful of the sample size with only 335 observations. If we split that over too many bins, the sample sizes in the individual kernels would be uncomfortably small for regression. The model has fail as the response, a kernel estimation of nctdel with 4 knots, and the features male, black, hispanic, and count_sn. The bin levels are (-Inf,0], (0,0.357], (0.357,0.737], (0.737,0.983], (0.983,1.21], (1.21,1.49], (1.49,1.73], (1.73,2.16], (2.16,2.8], (2.8, Inf]. A summary of the model noted some significance for the feature nctdel (for the full summary, Appendix C).

##Bins

```{r echo = F}
# set up the kernels
#bin <- cut(kelly$nctdel, c(-Inf,seq(0,26,.3),Inf))
quants <- quantile(kelly$nctdel, probs = seq(0,1,length.out = 10))
bin <- cut(kelly$nctdel, c(-1,quants))
levels(bin) <- seq(1:nlevels(bin))
```


```{r, echo = F}
# instantiate data
times <- as.numeric(bin)
total_bins <- max(times)
censored <- kelly$fail
charac <- kelly %>% select(-c(nctdel, fail))
# example transformation
# transformation function

transformation <- function(time, censored, total_bins) {
  y <- rep(0,time)
  if(!censored) {
    y[time] <- 1
  }
  
  # if you are doing kernel regression then you will calculate X differently here (using the kernels)
  X <- array(0, dim=c(time, total_bins))
  diag(X) <- 1
  
  return(data.frame(y,X))
}
# apply the transformation to the data set
d <- transformation(times[1], censored[1], total_bins)
for(i in 2:length(times)) {
  d <- rbind(d, transformation(times[i], censored[i], total_bins))
}
# fit a model
m <- glm(y ~ 0 + ., data=d, family="binomial")
summary(m)

beta <- coef(m)
hazard <- exp(beta)/(1+exp(beta))
plot(1:total_bins, hazard, type='l')

# this is pretty ugly. Let's smooth it out with kernel regression.
# set up the kernels
kernels <- 4
tau <- seq(from=1, to=total_bins, length.out = kernels)
sigma <- (tau[2]-tau[1])/2
# we're going to pre-calculate a bunch of kernel weights
# each row is one bin, the columns are the kernel weights for that bin
kernel.weights <- matrix(dnorm(rep(1:80,kernels),rep(tau,each=80),sigma), ncol=kernels)
# now we'll create a new transform function
kernel.transformation <- function(time, censored, other_cov, total_bins) {
  y <- rep(0,time)
  if(!censored) {
    y[time] <- 1
  }
  
  # X has kernel weights instead of the bin indicators that it had before
  X <- array(kernel.weights[1:time,],dim=c(time,kernels))
  X <- cbind(X, other_cov)
  
  return(data.frame(y,X))
}

d2 <- NULL
d2 <- kernel.transformation(times[1], censored[1], charac[1,],total_bins)
for(i in 2:length(times)) {
  d2 <- rbind(d2, kernel.transformation(times[i], censored[i], charac[i,], total_bins))
}

# fit a model
m2 <- glm(y ~ 0 + ., data=d2, family="binomial")
summary(m2)

```


##Model Checks
To understand how well the model fits, we performed goodness of fit tests and a model check assumptions.
```{r echo = F}
par(mfrow = c(2,2))
plot(m2)
```
Looking at the Residuals vs. Fitted graph, the points do not exhibit a pattern and are relatively evenly distributed. With the Residuals vs. Leverage plot, the points are not homoscedastically distributed. In addition, data no longer tends to be normal the longer the patient stays waiting, as shown in the Normal Q-Q plot. Generally, the model seems to decently fit our data and is exceptionally better than previous models.
```{r echo = F}
X.test <- kelly %>% select(male, black, hisp, count_sn)

X.test <- data.frame(cbind(kern_distribution(kelly$nctdel), X.test))


kern.pred <- round(predict.glm(m2, newdata = data.frame(X.test), type = 'response'))
m = mean(kern.pred != kelly$fail)
```
The model fits the true dataset `r m*100`% of the time when predicting for failure over the entire dataset, implying a decent model which holds much potential given the assumptions hold.


##Discussion
###CREATE HAZARD --> y is the binned time at failure
```{r}
##select rows in 
hazard.logodds <- predict(m2, newdata = data.frame(d2))

###change to assess different survival functions

hazard2 <- exp(hazard.logodds)/(1+exp(hazard.logodds))
plot(1:total_bins, hazard2, type='l')
# you can choose different numbers of kernels or even use un-evenly spaced kernels to capture more detail early on where you have more data.
```

##Gender

##Race

##Symptoms


#Recommendations
A kernal regression hazard model is dependent on all the observations; a slight change in some observations with high leverage could potentially change the model quite a bit. Reproducing this study on various differing datasets could prove beneficial in discovering a better model. It would be also be interesting to test different percentiles for the binning of the model. In addition, the bump in the hazard plot seems quite peculiar and the cases for those patients should be investigated further.