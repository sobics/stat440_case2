---
title: "Case 2 Part 3"
author: "Sonia Xu, Grant Goettel, Ian Hua"
date: "October 22, 2017"
output: html_document
---

```{r echo = F, warning = F, include = F}
knitr::opts_chunk$set(fig.width=5, fig.height=3) 
library(dplyr)
library(magrittr)
library(knitr)
library(ggplot2)
```


```{r include = F}
kelly <- read.table("kellydat.txt", header = T)
kelly <- kelly %>% mutate(count_sn = sn1 + sn2*2 + sn3*3 + all4*4) %>% select(-c(sn1, sn2, sn3, all4))
```

<!-- BACKGROUND: Study of time to critical neurological assessment for patients with stroke-like symptoms who are admitted to the emergency room.  A possible predictor is number of major stroke symptoms reported, ranging from 0 to 4.  Treatment for acute stroke includes thrombolytic therapy, which can potentially improve neurological functioning for ischemic stroke patients if administered soon after symptom onset (within 3 hours).  Since treating patients quickly is critically important for their long-term prognosis, minimizing the times from symptom onset to emergency room (ED) arrival, from ED arrival to diagnosis, and from diagnosis to treatment is of paramount concern.
INTEREST: Factors predictive of the time to critical neurological assessment following admission to the ED for n=335 patients with mild to moderate motor impairment.  The goal of the analysis is to perform inferences on the impact of clinical presentation, gender and race on time to neurological assessment.  Clinical presentation is measured as a count of reported major stroke symptoms, including headache, loss of motor skills or weakness, trouble talking or understanding, and vision problems.
-->
#Introduction
The following data is from a study of time to critical neurological assessment for patients with stroke-like symptoms who were admitted to the emergency room. The purpose of the analysis is to perform inferences on the impact of clinical presentation (reported number of makor stroke symptoms), gender, and race on time to neurological asessment. This paper fits a model that identifies the differences in wait time to neurological assessment based on these features of interest. A final model of kernel regression with a response of fail, continuous feature of nctdel estimated via kernel regression, and binary features of black, hispanic, male, number of symptoms.


#Methodology
```{r}

```

#Data set
The data set contained information about the amount of time elapsed from arrival at the ER to the assessment, and whether or not the patient received a CT scan. It also contained data about whether each patient was male or female, whether they were black or hispanic or not, and the amount of major symptoms of a stroke they had upon arrival (out of 4 main symptoms). Rather than have four separate indicator variables showing whether they had 1, 2, 3, or 4 symptoms, we created a single numerical variable containing the amount of symptoms.

#EDA

##Racial Bias
###Black vs. Non-Black
There could potentially be bias here, but more analysis should be done. Non-blacks appear to have shorter wait times, but some non-blacks have extremely long wait times.

```{r echo = F}

ggplot(kelly, aes(nctdel, group = factor(black), fill = factor(black,labels=c("no", "yes"))),alpha=0.5, adjust=2) + geom_density() + theme_bw() + xlab("Wait Time") + ylab("Frequency of Wait Time") + labs(fill = "Black?") + ggtitle("Differences in Wait Time if Black")
```

###Hispanic vs. Non-Hispanic
There could potentially be bias here, but the lack of sample size for Hispanics appears to be an issue.
```{r echo = F}
ggplot(kelly, aes(nctdel, group = factor(hisp), fill = factor(hisp,labels=c("no", "yes"))),alpha=0.5, adjust=2) + geom_density() + theme_bw() + xlab("Wait Time") + ylab("Frequency of Wait Time") + labs(fill = "Hispanic?") + ggtitle("Differences in Wait Time if Hispanic")
```


##Gender Bias
There could potentially be bias here, but more analysis should be done. Females appear to have shorter wait times.

```{r echo = F}
ggplot(kelly, aes(nctdel, group = factor(male), fill = factor(male,labels=c("no", "yes"))),alpha=0.5, adjust=2) + geom_density() + theme_bw() + xlab("Wait Time") + ylab("Frequency of Wait Time") + labs(fill = "Male?") + ggtitle("Differences in Wait Time if Male")
```

#Final Model

We decided to create a Kernel Regression model with failure as the response to explore the robustness of this new model.

##Kernel Regression with 10 Bins
13 bins were calculated to fit the kernels. The bins are unevenly spaced because the data has a higher concentration of points for the feature nctdel between 0 and 2, even though its range is (0,26.25). We chose 10 bins because we wanted to be careful of the sample size with only 335 observations. If we split that over too many bins, the sample sizes in the individual kernels would be uncomfortably small for regression. The model has fail as the response, a kernel estimation of nctdel with 4 knots, and the features male, black, hispanic, and count_sn. The bin levels are (-Inf,0], (0,0.3], (0.3,0.7], (0.7,1], (1,1.1], (1.1,1.3], (1.3,1.6], (1.6,1.9], (1.9,10], (10,13], (13,15], (17, Inf]. A summary of the model noted some significance for the feature nctdel (for the full summary, Appendix C).

##binning the data

```{r echo = F}
# set up the kernels
#bin <- cut(kelly$nctdel, c(-Inf,seq(0,26,.3),Inf))
quants <- quantile(kelly$nctdel, probs = seq(0,1,length.out = 11))
bin <- cut(kelly$nctdel, c(-1,quants))
levels(bin) <- seq(1:nlevels(bin))
#bins <- cut(kelly$nctdel, c(-Inf,0,0.5, 0.7, 1.3, 2.05,10,15,Inf)) #manually choose bins based on quantiles and my own knowledge
# we're going to pre-calculate a bunch of kernel weights
# each row is one bin, the columns are the kernel weights for that bin
```


<<<<<<< HEAD
```{r echo = F, ignore = T}
#trial and error
kernel.weights <- as.matrix(kelly %>% group_by(bins) %>% summarise(mean(nctdel),mean(male), mean(black), mean(hisp), mean(count_sn)))

#take 2
attach(kelly)
kernel.weights_2 <- data.frame(cbind(bin,kern_distribution(log(nctdel+1)), (kern_distribution(male)), (kern_distribution(black)), (kern_distribution(hisp))))

kernel.weights <- kernel.weights_2 %>% group_by(bin) %>% summarise(mean(V2), mean(V3), mean(V4), mean(V5), mean(V6), mean(V7), mean(V8), mean(V9), mean(V10), mean(V11), mean(V12), mean(V13), mean(V14), mean(V15),mean(V16), mean(V17))
    
detach(kelly)
```


```{r echo = F}
#take 3
attach(kelly)
kernel.weights_3 <- data.frame(cbind(bin,kern_distribution(log(nctdel+1)), male, black, hisp))
kernel.weights <- kernel.weights_3 %>% group_by(bin) %>% summarise(mean(V2), mean(V3), mean(V4), mean(V5), mean(male), mean(black), mean(hisp), mean(count_sn))

colnames(kernel.weights) <- c("bin","X1","X2", "X3", "X4", "male", "black", "hisp", "count_sn")

#Edited model code
```{r}
# instantiate data
times <- as.numeric(bin)
total_bins <- max(times)
censored <- kelly$fail
charac <- kelly %>% select(-c(nctdel, fail))
# example transformation
# transformation function

transformation <- function(time, censored, total_bins) {
  y <- rep(0,time)
  if(!censored) {
    y[time] <- 1
  }
  
  # if you are doing kernel regression then you will calculate X differently here (using the kernels)
  X <- array(0, dim=c(time, total_bins))
  diag(X) <- 1
  
  return(data.frame(y,X))
}
# apply the transformation to the data set
d <- transformation(times[1], censored[1], total_bins)
for(i in 2:length(times)) {
  d <- rbind(d, transformation(times[i], censored[i], total_bins))
}
# fit a model
m <- glm(y ~ 0 + ., data=d, family="binomial")
summary(m)

beta <- coef(m)
hazard <- exp(beta)/(1+exp(beta))
plot(1:total_bins, hazard, type='l')
# this is pretty ugly. Let's smooth it out with kernel regression.
# set up the kernels
kernels <- 4
tau <- seq(from=1, to=total_bins, length.out = kernels)
sigma <- (tau[2]-tau[1])/2
# we're going to pre-calculate a bunch of kernel weights
# each row is one bin, the columns are the kernel weights for that bin
kernel.weights <- matrix(dnorm(rep(1:80,kernels),rep(tau,each=80),sigma), ncol=kernels)
# now we'll create a new transform function
kernel.transformation <- function(time, censored, other_cov, total_bins) {
  y <- rep(0,time)
  if(!censored) {
    y[time] <- 1
  }
  
  # X has kernel weights instead of the bin indicators that it had before
  X <- array(kernel.weights[1:time,],dim=c(time,kernels))
  X <- cbind(X, other_cov)
  
  return(data.frame(y,X))
}

d2 <- NULL
d2 <- kernel.transformation(times[1], censored[1], charac[1,],total_bins)
for(i in 2:length(times)) {
  d2 <- rbind(d2, kernel.transformation(times[i], censored[i], charac[i,], total_bins))
}

# fit a model
m2 <- glm(y ~ 0 + ., data=d2, family="binomial")
summary(m2)
# make a smooth plot

```


##Model Checks
To understand how well the model fits, we performed goodness of fit tests and a model check assumptions.
```{r echo = F}
par(mfrow = c(2,2))
plot(m2)
```
Looking at the Residuals vs. Fitted Graph, the points do not exhibit a pattern. However, the points are not evenly distrbuted, so they are heteroscedastic. Similarly, with Residuals vs. Leverage, the points are not homoscedastically distributed. While this model better fits the data than previous models, this model can also be improved. 
```{r echo = F}
X.test <- kelly %>% select(male, black, hisp, count_sn)

X.test <- data.frame(cbind(kern_distribution(kelly$nctdel), X.test))


kern.pred <- round(predict.glm(m2, newdata = data.frame(X.test), type = 'response'))
m = mean(kern.pred != kelly$fail)
```
Overall, the model fits the true dataset `r m*100`% of the time when predicting for failure over the entire dataset, which reaffirms the fact that the model can be improved. 


##Discussion
###CREATE HAZARD --> y is the binned time at failure
```{r}
##select rows in 
hazard.logodds <- predict(m2, newdata = data.frame(d2))


###change to assess different survival functions


hazard <- exp(hazard.logodds)/(1+exp(hazard.logodds))
plot(1:total_bins, hazard, type='l')
# you can choose different numbers of kernels or even use un-evenly spaced kernels to capture more detail early on where you have more data.
```

##Gender

##Race

##Symptoms


#Recommendations


