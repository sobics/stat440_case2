---
title: "Case 2 Part 3"
author: "Sonia Xu"
date: "October 22, 2017"
output: html_document
---

<!-- BACKGROUND: Study of time to critical neurological assessment for patients with stroke-like symptoms who are admitted to the emergency room.  A possible predictor is number of major stroke symptoms reported, ranging from 0 to 4.  Treatment for acute stroke includes thrombolytic therapy, which can potentially improve neurological functioning for ischemic stroke patients if administered soon after symptom onset (within 3 hours).  Since treating patients quickly is critically important for their long-term prognosis, minimizing the times from symptom onset to emergency room (ED) arrival, from ED arrival to diagnosis, and from diagnosis to treatment is of paramount concern.
INTEREST: Factors predictive of the time to critical neurological assessment following admission to the ED for n=335 patients with mild to moderate motor impairment.  The goal of the analysis is to perform inferences on the impact of clinical presentation, gender and race on time to neurological assessment.  Clinical presentation is measured as a count of reported major stroke symptoms, including headache, loss of motor skills or weakness, trouble talking or understanding, and vision problems.
-->
#Introduction
The following data is from a study of time to critical neurological assessment for patients with stroke-like symptoms who were admitted to the emergency room. The purpose of the analysis is to perform inferences on the impact of clinical presentation (reported number of makor stroke symptoms), gender, and race on time to neurological asessment. This paper fits a model that identifies the differences in wait time to neurological assessment based on these features of interest. ##FINAL MODEL SENTENCE 


#Methodology
```{r}

```

#Dataset
```{r}
```

#Final Model

However, when changing the response to failure (0/1), the most significant predictors are nctdel, count_sn, and male. Being hispanic is less important.

Originally, for the CPH model and KM model, we used nctdel as the response, and noticed goodness-of-fit issues. We decided to create a Kernel Regression model with failure as the response to explore the robustness of this new model.

##Kernel Regression with 13 Bins
13 bins were calculated to fit the kernels. The bins are unevenly spaced because the data has a higher concentration of points for the feature nctdel between 0 and 2, even though its range is (0,26.25). The model has fail as the response, a kernel estimation of nctdel with 4 knots, and the features male, black, hispanic, and count_sn. The bin levels are (-Inf,0], (0,0.3], (0.3,0.7], (0.7,1], (1,1.1], (1.1,1.3], (1.3,1.6], (1.6,1.9], (1.9,10], (10,13], (13,15], (17, Inf]. A summary of the model noted some significance for the feature nctdel (for the full summary, Appendix C).
```{r echo = F}
# set up the kernels
#bin <- cut(kelly$nctdel, c(-Inf,seq(0,26,.3),Inf))
bin <- cut(kelly$nctdel, c(-Inf,0,0.3,0.7,1,1.1,seq(1.3,2.05,0.3),10,13,15,17,Inf))
#bins <- cut(kelly$nctdel, c(-Inf,0,0.5, 0.7, 1.3, 2.05,10,15,Inf)) #manually choose bins based on quantiles and my own knowledge
levels(bin) <- as.character(seq(1:nlevels(bin)))
kelly$bins = bin
# we're going to pre-calculate a bunch of kernel weights
# each row is one bin, the columns are the kernel weights for that bin
```

```{r echo = F, ignore = T}
#trial and error
kernel.weights <- as.matrix(kelly %>% group_by(bins) %>% summarise(mean(nctdel),mean(male), mean(black), mean(hisp), mean(count_sn)))

#take 2
attach(kelly)
kernel.weights_2 <- data.frame(cbind(bin,kern_distribution(log(nctdel+1)), (kern_distribution(male)), (kern_distribution(black)), (kern_distribution(hisp))))

kernel.weights <- kernel.weights_2 %>% group_by(bin) %>% summarise(mean(V2), mean(V3), mean(V4), mean(V5), mean(V6), mean(V7), mean(V8), mean(V9), mean(V10), mean(V11), mean(V12), mean(V13), mean(V14), mean(V15),mean(V16), mean(V17))

detach(kelly)
```


```{r echo = F}
#take 3
attach(kelly)
kernel.weights_3 <- data.frame(cbind(bin,kern_distribution(log(nctdel+1)), male, black, hisp))
kernel.weights <- kernel.weights_3 %>% group_by(bin) %>% summarise(mean(V2), mean(V3), mean(V4), mean(V5), mean(male), mean(black), mean(hisp), mean(count_sn))

colnames(kernel.weights) <- c("bin","X1","X2", "X3", "X4", "male", "black", "hisp", "count_sn")

# now we'll create a new transform function
kernel.transformation <- function(time, censored) {
  y <- rep(0,time)
  if(!censored) {
    y[time] <- 1
  }
  
  # X has kernel weights instead of the bin indicators that it had before
  X <- (kernel.weights[1:time,-1])#,dim=c(time,ncol(kernel.weights)-1)) ###how to translate this line??
  
  return(data.frame(y,X))
}

d2 <- kernel.transformation((as.numeric(kelly$bins)[1]), as.logical(kelly$fail[1]))

d2_full <- NULL
for(i in 1:length(kelly$nctdel)) {
  d2_full <- rbind(d2_full,kernel.transformation(as.numeric(bins)[i], as.logical(kelly$fail[i])))
}
```

```{r echo = F, warning=F}
# fit a model

m2 <- glm(y ~ 0 + ., data=data.frame(d2_full), family="binomial")
#summary(m2)

# make a smooth plot
```

```{r echo = F}
X.pred <- kernel.weights[,-1]

hazard.logodds <- predict(m2, newdata = data.frame(X.pred), type = "response")

hazard <- exp(hazard.logodds)/(1+exp(hazard.logodds))
plot(1:nlevels(bin), hazard, type='l')

# you can choose different numbers of kernels or even use un-evenly spaced kernels to capture more detail early on where you have more data.
```
Looking at the hazard plot, after bin 6 (1.1,1.3], the survival log odds are significantly lower.


##Model Checks
To understand how well the model fits, we performed goodness of fit tests and a model check assumptions.
```{r echo = F}
par(mfrow = c(2,2))
plot(m2)
```
Looking at the Residuals vs. Fitted Graph, the points do not exhibit a pattern. However, the points are not evenly distrbuted, so they are heteroscedastic. Similarly, with Residuals vs. Leverage, the points are not homoscedastically distributed. While this model better fits the data than previous models, this model can also be improved. 
```{r echo = F}
X.test <- kelly %>% select(male, black, hisp, count_sn)

X.test <- data.frame(cbind(kern_distribution(kelly$nctdel), X.test))



kern.pred <- round(predict.glm(m2, newdata = data.frame(X.test), type = 'response'))
m = mean(kern.pred != kelly$fail)
```
Overall, the model fits the true dataset `r m*100`% of the time when predicting for failure over the entire dataset, which reaffirms the fact that the model can be improved. 

##EDA
```{r}
```

##Goodness of Fit Test
```{r}
```

##Discussion
```{r}
```

