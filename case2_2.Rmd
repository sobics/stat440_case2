---
title: "Case 2 Part 2"
author: "Sonia Xu, Grant Goettel, Ian Hua"
date: "October 14, 2017"
output:
  pdf_document: default
  html_document: default
---

<!--- Week 2: Report #1 on the case study introduced the previous week is due by Wed class time.  These reports should be done using LaTeX or RMarkdown and submitted electronically. They should be clear, concise and contain plots and descriptions of the exploratory/descriptive analyses along with an analysis plan including relevant references.  The lecture time will be devoted to providing details on statistical approaches relevant to the case study, and will not have time for the teams to work together or be a detailed question/answer session (Mondayâ€™s class is devoted to such things).  It is expected that the teams will devote substantial time out of class.

CASE STUDY 2: 

BACKGROUND: Study of time to critical neurological assessment for patients with stroke-like symptoms who are admitted to the emergency room.  A possible predictor is number of major stroke symptoms reported, ranging from 0 to 4.  Treatment for acute stroke includes thrombolytic therapy, which can potentially improve neurological functioning for ischemic stroke patients if administered soon after symptom onset (within 3 hours).  Since treating patients quickly is critically important for their long-term prognosis, minimizing the times from symptom onset to emergency room (ED) arrival, from ED arrival to diagnosis, and from diagnosis to treatment is of paramount concern.

INTEREST: Factors predictive of the time to critical neurological assessment following admission to the ED for n=335 patients with mild to moderate motor impairment.  The goal of the analysis is to perform inferences on the impact of clinical presentation, gender and race on time to neurological assessment.  Clinical presentation is measured as a count of reported major stroke symptoms, including headache, loss of motor skills or weakness, trouble talking or understanding, and vision problems.

Variable key: 
nctdel = min of neurologist time to assessment & CT scan from arrival at ER
fail = 1 if got neurologist/CT scan & 0 otherwise
male = 1 if male, 0=female
black = 1 if black, 0=not black 
hisp = 1 if hispanic, 0=not hispanic
sn1 = 0/1 indicator 1 main symptom
sn2 = 0/1 indicator 2 main symptoms
sn3 = 0/1 indicator 3 main symptoms 
all4 = 0/1 indicator all 4 main symptoms
--->
```{r include = F}
#load libraries and data

library(survival)
library(survminer)
library(dplyr)
library(cluster)
library(randomForest)

kelly <- read.table("kellydat.txt", header = T)
kelly <- kelly %>% mutate(count_sn = sn1 + sn2*2 + sn3*3 + all4*4) %>% select(-c(sn1, sn2, sn3, all4))
```
#Overview

Multiple models and analyses were explored to find the best model that fit the neurological assessment data. Methods explored included:

1. Cox Proportional Hazards Model

2. K-Means Clustering

3. Random Forest

4. Kernel Regression

#Exploring the Cox Proportional Hazards Model
```{r}
res.cox <- coxph(Surv(nctdel, fail) ~ male + black + hisp + factor(count_sn), data =  kelly)

summary(res.cox)
```
Looking at the Cox Model, only one of the coefficients are significant in detecting the nctdel waiting time--if the number of symptoms = 4. Either the data is not informative or the model is not a good fit of the data.

To assess the goodness of fit of the model, we check to see if the model fits the Cox Proportional Hazards Model Assumptions of no influential outliers, linearity, and homoscedascity of Schoenfeld residuals. The dataset does not satisfy the linearity assumption, so this implies that the Cox Proportional Hazard Model may not be the best model for the dataset.    
##Schoenfeld Residuals
```{r echo = F}
fit.cox <- cox.zph(res.cox)
ggcoxzph(fit.cox)
```
From the graphical inspection, there exists a pattern (slight curve in tails) with time for the feature black. The assumption of proportional hazards appears to be supported for the covariates male, each factor of the symptoms, and hispanic.
##Test for Outliers
```{r}
ggcoxdiagnostics(res.cox, type = "dfbeta",
                 linear.predictions = FALSE, ggtheme = theme_bw())
```
Most of the plots show no trends, so no points are significantly influential.
##Linearity
```{r echo = F}
ggcoxfunctional(Surv(nctdel, fail) ~ count_sn+1 + log(count_sn+1) + sqrt(count_sn+1), data = kelly)
```
The feature count_sn does not follow a linear trend, so it breaks the linearity assumption for a CPH Model.



#plot of the baseline survival function
```{r}
ggsurvplot(survfit(res.cox), color = "#2E9FDF",
           ggtheme = theme_minimal()) + geom_vline(xintercept=2.5, linetype="dashed", color = "red", size=1)
```

Looking at the baseline survival function, the survival probability of patients drastically decrease after 2.5 minutes. 


#Clustering the Data
```{r}
set.seed(3)
kCluster <- kmeans(kelly[,-1], 3, nstart = 20)
kCluster$cluster <- as.factor(kCluster$cluster)
ggplot(kelly, aes(hisp, nctdel, color = kCluster$cluster)) + geom_point()
```

#Random Forest
```{r}
rf.randomForest <- randomForest(nctdel ~., kelly)
varImpPlot(rf.randomForest)
```
Based on the variable importance plot, the most significant variables for determining nctdel wait time are in the order: fail, hispanic, count of symptoms, sex, and black.

##Kaplan-Meier Estimate
```{r warning = F, echo = F}
library(survival)
d2 <- Surv(kelly$nctdel, kelly$fail) #creates a survival object #preprocessed version of the data

# km for male/female, red=female, blue=male
kmM <- survfit(d2 ~ kelly$male)
plot(kmM, col = c("red","blue"))
print(kmM)

# km for black/non-black, red=nonblack, blue=black
kmB <- survfit(d2 ~ kelly$black)
plot(kmB, col = c("red","blue"))
print(kmB)

# km for hispanic/non-hispanic, red=nonhisp, blue=hisp
kmH <- survfit(d2 ~ kelly$hisp)
plot(kmH, col = c("red","blue"))
print(kmH)

# KM for count of symptoms, red=0 to blue=5
kmCOS <- survfit(d2 ~ kelly$count_sn)
plot(kmCOS, col = c("red","orange","yellow","green","blue"))
print(kmCOS)
```
The Kaplan-Meier estimate can take into account the fail variable properly in the dataset. It is clear that sample size is an issue for some of categories but for the most part we can observe general relationships between categories in our dataset. It seems that females are treated slightly faster than males although not significant. Non-blacks appear to be treated faster than blacks with a 95% CI for non-blacks between 1.33 and 1.62 while a 95% CI for blacks is between 1.68 and 2.08. This appears to be significant bias, however there appears to be no bias with regard to hispanics. Finally, by looking at the graph for the Kaplan-Meier estimate on the dataset based on count of symptoms, we observe that symptom count does appear to be a major factor in wait time, especially when all four symptoms are present. 

The following is the Kaplan-Meier estimate for all subcategories present in the data set. Larger sample sizes would be useful.
```{r}
km <- survfit(d2 ~  kelly$male + kelly$black + kelly$hisp + kelly$count_sn)
print(km)
```

#Discrete Regression Approach

#how to compare the two models to see which one performs better??

cox: prorportional hazards model
glm: proportional log odds model


```{r}
m <- glm(fail ~ kern_distribution(nctdel, knots = 2) + male + black + hisp + count_sn, data = kelly, family = "binomial")
plot(kelly$nctdel,kelly$fail, col = "red", main = "nctdel vs. fail", ylab = "fail", xlab = "nctdel") #plots the true data
lines(sort(kelly$nctdel), fitted(m)[order(kelly$fail)]) #shows how well the model fits the true data

#for xs, time bin tells us how far the person went
#-for the censored, they dont know what happened after the x bin (0/1)
#-matrix for each individual
```

```{r}
# fake data
times <- kelly$nctdel #1 + c(rgeom(20, .3), rgeom(20,.05))
total_bins <- max(times)

censored <- rbinom(40, 1, .2)

# example transformation
# transformation function

transformation <- function(time, censored, total_bins, kernel.weights = bins) {
  y <- rep(0,time)
  if(!censored) {
    y[time] <- 1
  }
  # if you are doing kernel regression then you will calculate X differently here (using the kernels)
   X <- array(kernel.weights[1:time,],dim=c(time,kernels))
  diag(X) <- 1
  return(data.frame(y,X))
}

d <- transformation(times[1], as.logical(kelly$fail)[1], total_bins)


for(i in 2:length(times)) {
  d <- rbind(d, transformation(times[i], as.logical(kelly$fail)[i], total_bins))
}

m <- glm(y ~ 0 + ., data = d, family = "binomial")
summary(m)

#if estimates are negative, then you have TOOOOOOO MANYYYY BINS--beware, you can either 1. reduce # of bins or 2. use kernel regression

#ask how to do kernel regression to smooth out empty bins

bins <- cut(kelly$nctdel, c(-Inf, 0.5, 1.3, 2.05,10,15,Inf))
levels(bins) <- c("1","2","3","4","5", "6")
kelly$bins = bins

######
beta <- coef(m)
hazard <- exp(beta)/(1+exp(beta))
plot(1:total_bins, hazard, type='l')
```
This is pretty ugly. Let's smooth it out with kernel regression.

```{r}
# set up the kernels

bins <- cut(kelly$nctdel, c(-Inf, 0.5, 1.3, 2.05,10,15,Inf))
levels(bins) <- c("1","2","3","4","5", "6")
kelly$bins = bins
# we're going to pre-calculate a bunch of kernel weights
# each row is one bin, the columns are the kernel weights for that bin
kernel.weights <- matrix(dnorm(rep(1:80,kernels),rep(tau,each=80),sigma), ncol=kernels)
kernel.weights <- as.matrix(kelly %>% group_by(bins) %>% summarise(mean(nctdel),mean(male), mean(black), mean(hisp), mean(count_sn)))
# now we'll create a new transform function
kernel.transformation <- function(time, censored, total_bins) {
  y <- rep(0,time)
  if(!censored) {
    y[time] <- 1
  }
  
  # X has kernel weights instead of the bin indicators that it had before
  X <- array(as.numeric(kernel.weights[1:time,-1]),dim=c(time,ncol(kernel.weights)-1)) ###how to translate this line??
  
  return(data.frame(y,X))
}

d2 <- kernel.transformation(as.numeric(bins)[1], as.logical(kelly$fail[1]), nlevels(bins))
d2 <- kernel.transformation(kelly$nctdel[1], as.logical(kelly$fail[1]), nrows(kelly), ncol(kernel.weights))

d2_full <- NULL
for(i in 2:length(kelly$nctdel)) {
  d2_full <- rbind(d2_full,kernel.transformation(as.numeric(bins)[i], as.logical(kelly$fail[i]), nlevels(bins)))
}
```

```{r}
# fit a model

m2 <- glm(y ~ 0 + ., data=data.frame(d2_full), family="binomial")
summary(m2)

# make a smooth plot

X.pred <- array(as.numeric(kernel.weights[,-1]),dim=c(6,ncol(kernel.weights)-1))

hazard.logodds <- predict(m2, newdata =X.pred)
hazard <- exp(hazard.logodds)/(1+exp(hazard.logodds))
plot(1:total_bins, hazard, type='l')

# you can choose different numbers of kernels or even use un-evenly spaced kernels to capture more detail early on where you have more data.
```