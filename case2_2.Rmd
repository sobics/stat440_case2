---
title: "Case 2 Part 2"
author: "Sonia Xu, Grant Goettel, Ian Hua"
date: "October 14, 2017"
output: 
  pdf_document: default
  html_document: default
---

<!--- Week 2: Report #1 on the case study introduced the previous week is due by Wed class time.  These reports should be done using LaTeX or RMarkdown and submitted electronically. They should be clear, concise and contain plots and descriptions of the exploratory/descriptive analyses along with an analysis plan including relevant references.  The lecture time will be devoted to providing details on statistical approaches relevant to the case study, and will not have time for the teams to work together or be a detailed question/answer session (Mondayâ€™s class is devoted to such things).  It is expected that the teams will devote substantial time out of class.

CASE STUDY 2: 

BACKGROUND: Study of time to critical neurological assessment for patients with stroke-like symptoms who are admitted to the emergency room.  A possible predictor is number of major stroke symptoms reported, ranging from 0 to 4.  Treatment for acute stroke includes thrombolytic therapy, which can potentially improve neurological functioning for ischemic stroke patients if administered soon after symptom onset (within 3 hours).  Since treating patients quickly is critically important for their long-term prognosis, minimizing the times from symptom onset to emergency room (ED) arrival, from ED arrival to diagnosis, and from diagnosis to treatment is of paramount concern.

INTEREST: Factors predictive of the time to critical neurological assessment following admission to the ED for n=335 patients with mild to moderate motor impairment.  The goal of the analysis is to perform inferences on the impact of clinical presentation, gender and race on time to neurological assessment.  Clinical presentation is measured as a count of reported major stroke symptoms, including headache, loss of motor skills or weakness, trouble talking or understanding, and vision problems.

Variable key: 
nctdel = min of neurologist time to assessment & CT scan from arrival at ER
fail = 1 if got neurologist/CT scan & 0 otherwise
male = 1 if male, 0=female
black = 1 if black, 0=not black 
hisp = 1 if hispanic, 0=not hispanic
sn1 = 0/1 indicator 1 main symptom
sn2 = 0/1 indicator 2 main symptoms
sn3 = 0/1 indicator 3 main symptoms 
all4 = 0/1 indicator all 4 main symptoms
--->
```{r include = F}
#load libraries and data

library(survival)
library(survminer)
library(dplyr)
library(cluster)
library(randomForest)

kelly <- read.table("kellydat.txt", header = T)
kelly <- kelly %>% mutate(count_sn = sn1 + sn2*2 + sn3*3 + all4*4) %>% select(-c(sn1, sn2, sn3, all4))
```
#Overview

Multiple models and analyses were explored to find the best model that fit the neurological assessment data. Methods explored included:

1. Cox Proportional Hazards Model

2. K-Means Clustering

3. Random Forest

4. Kernel Regression

#Exploring the Cox Proportional Hazards Model
```{r}
res.cox <- coxph(Surv(nctdel, fail) ~ male + black + hisp + factor(count_sn), data =  kelly)

summary(res.cox)
```
Looking at the Cox Model, only one of the coefficients are significant in detecting the nctdel waiting time--if the number of symptoms = 4. Either the data is not informative or the model is not a good fit of the data.

To assess the goodness of fit of the data, we check to see if the model fits the Cox Proportional Hazards 
```{r}
fit.cox <- cox.zph(res.cox)
plot(fit.cox)
```

#plot of the baseline survival function
```{r}
ggsurvplot(survfit(res.cox), color = "#2E9FDF",
           ggtheme = theme_minimal()) + geom_vline(xintercept=2.5, linetype="dashed", color = "red", size=1)
```

Looking at the baseline survival function, the survival probability of patients drastically decrease after 2.5 minutes. 


#Clustering the Data
```{r}
set.seed(3)
kCluster <- kmeans(kelly[,-1], 3, nstart = 20)
kCluster$cluster <- as.factor(kCluster$cluster)
ggplot(kelly, aes(hisp, nctdel, color = kCluster$cluster)) + geom_point()
```

#Random Forest
```{r}
rf.randomForest <- randomForest(nctdel ~., kelly)
varImpPlot(rf.randomForest)
```
Based on the variable importance plot, the most significant variables for determining nctdel wait time are in the order: fail, hispanic, count of symptoms, sex, and black.

##Kaplan-Meier Estimate
```{r warning = F, echo = F}
library(survival)
d2 <- Surv(kelly$nctdel, kelly$fail) #creates a survival object #preprocessed version of the data
km <- survfit(d2 ~ kelly$male + kelly$black + kelly$hisp + kelly$count_sn)
plot(km, col = c("red", "blue", "green", "yellow", "purple", "pink", "orange", "brown"))
#print(km)
```


#Discrete Regression Approach

#how to compare the two models to see which one performs better??

cox: prorportional hazards model
glm: proportional log odds model

#kernel regression
```{r include = F}
#creates a kernel distribution for each covariate
#be careful of random samples--Big X can't have NAs
kern_distribution <- function(x,knots = 4) {
  ### s: #controls how wide the kernels are, s should be half the distance between two knots 
  ### tau: decides where the breaks are based on the number of knots
  ###knots: number of splits/peaks in the data
  ##x and y are the data
  
  tau <- seq(min(x), max(x), length.out = knots)
  s <- diff(tau)[1]/2
  X <- matrix(0, nrow=length(x),ncol=knots)
  for(i in 1:knots) {
    X[,i] <- x * dnorm(x, tau[i], s)
  }
  return(X)
}

#smoothing out the graph by predicting values for the fitted data
#same function as the kern distribution, but this time it is smoother
smooth_graph <- function(x,knots = 4) {
  tau <- seq(min(x), max(x), length.out = knots)
  s <- diff(tau)[1]/2
  x.predict <- seq(min(x), max(x), length.out=length(x))
  X.predict <- matrix(0, nrow=length(x.predict),ncol = knots)
  for(i in 1:knots) {
    X.predict[,i] <- x.predict * dnorm(x.predict, tau[i], s)
  }
  return(X.predict)
}

```


```{r}
m <- glm(fail ~ kern_distribution(nctdel, knots = 2) + male + black + hisp + count_sn, data = kelly, family = "binomial")
plot(kelly$nctdel,kelly$fail, col = "red", main = "nctdel vs. fail", ylab = "fail", xlab = "nctdel") #plots the true data
lines(sort(kelly$nctdel), fitted(m)[order(kelly$fail)]) #shows how well the model fits the true data

#for xs, time bin tells us how far the person went
-for the censored, they dont know what happened after the x bin (0/1)
-matrix for each individual

```{r}
# fake data
times <- kelly$nctdel #1 + c(rgeom(20, .3), rgeom(20,.05))
total_bins <- max(times)

censored <- rbinom(40, 1, .2)

# example transformation
# transformation function

transformation <- function(time, censored, total_bins) {
  y <- rep(0,time)
  if(!censored) {
    y[time] <- 1
  }
  # if you are doing kernel regression then you will calculate X differently here (using the kernels)
  X <- array(0, dim=c(time, total_bins))
  diag(X) <- 1
  return(data.frame(y,X))
}

d <- transformation(times[1], as.logical(kelly$fail)[1], total_bins)


for(i in 2:length(times)) {
  d <- rbind(d, transformation(times[i], as.logical(kelly$fail)[i]), total_bins)
}

m <- glm(y ~ 0 + ., data = d, family = "binomial")
summary(m)

#if estimates are negative, then you have TOOOOOOO MANYYYY BINS--beware, you can either 1. reduce # of bins or 2. use kernel regression

#ask how to do kernel regression to smooth out empty bins

bins <- cut(kelly$nctdel, c(-Inf, 0.5, 1.3, 2.05,10,15,Inf))


######
beta <- coef(m)
hazard <- exp(beta)/(1+exp(beta))
plot(1:total_bins, hazard, type='l')

# this is pretty ugly. Let's smooth it out with kernel regression.

# set up the kernels
kernels <- 4
tau <- seq(from=1, to=total_bins, length.out = kernels)
sigma <- (tau[2]-tau[1])/2

# we're going to pre-calculate a bunch of kernel weights
# each row is one bin, the columns are the kernel weights for that bin
kernel.weights <- matrix(dnorm(rep(1:80,kernels),rep(tau,each=80),sigma), ncol=kernels)
kernel.weights <- kelly %>% select(-(fail))
# now we'll create a new transform function
kernel.transformation <- function(time, censored, total_bins, kernels, i) {
  time = min(time, 1)
  y <- rep(0,time)
  if(!censored) {
    y[time] <- 1
  }
  
  # X has kernel weights instead of the bin indicators that it had before
  X <- array(kernel.weights[1:time,],dim=c(time,kernels)) ###how to translate this line??
  
  return(data.frame(y,X))
}

d2 <- kernel.transformation(times[1], censored[1], 80, kernels)
d2 <- kernel.transformation(kelly$nctdel[1], as.logical(kelly$fail[1]), nrows(kelly), ncol(kernel.weights))

for(i in 2:length(kelly$nctdel)) {
  print(i)
  d2 <- rbind(d2, kernel.transformation(kelly$nctdel[i], as.logical(kelly$fail[i]), total_bins = nrow(kelly), kernels = ncol(kelly)-1, i))
}

# fit a model

m2 <- glm(y ~ 0 + ., data=d2, family="binomial")
summary(m2)

# make a smooth plot


hazard.logodds <- predict(m2, newdata = data.frame(kernel.weights))
hazard <- exp(hazard.logodds)/(1+exp(hazard.logodds))
plot(1:total_bins, hazard, type='l')

# you can choose different numbers of kernels or even use un-evenly spaced kernels to capture more detail early on where you have more data.
```