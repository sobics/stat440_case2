---
title: "Case 2 Part 2"
author: "Sonia Xu, Grant Goettel, Ian Hua"
date: "October 18, 2017"
output:
  pdf_document: default
  html_document: default
---

<!--- Week 2: Report #1 on the case study introduced the previous week is due by Wed class time.  These reports should be done using LaTeX or RMarkdown and submitted electronically. They should be clear, concise and contain plots and descriptions of the exploratory/descriptive analyses along with an analysis plan including relevant references.  The lecture time will be devoted to providing details on statistical approaches relevant to the case study, and will not have time for the teams to work together or be a detailed question/answer session (Mondayâ€™s class is devoted to such things).  It is expected that the teams will devote substantial time out of class.

CASE STUDY 2: 

BACKGROUND: Study of time to critical neurological assessment for patients with stroke-like symptoms who are admitted to the emergency room.  A possible predictor is number of major stroke symptoms reported, ranging from 0 to 4.  Treatment for acute stroke includes thrombolytic therapy, which can potentially improve neurological functioning for ischemic stroke patients if administered soon after symptom onset (within 3 hours).  Since treating patients quickly is critically important for their long-term prognosis, minimizing the times from symptom onset to emergency room (ED) arrival, from ED arrival to diagnosis, and from diagnosis to treatment is of paramount concern.

INTEREST: Factors predictive of the time to critical neurological assessment following admission to the ED for n=335 patients with mild to moderate motor impairment.  The goal of the analysis is to perform inferences on the impact of clinical presentation, gender and race on time to neurological assessment.  Clinical presentation is measured as a count of reported major stroke symptoms, including headache, loss of motor skills or weakness, trouble talking or understanding, and vision problems.

Variable key: 
nctdel = min of neurologist time to assessment & CT scan from arrival at ER
fail = 1 if got neurologist/CT scan & 0 otherwise
male = 1 if male, 0=female
black = 1 if black, 0=not black 
hisp = 1 if hispanic, 0=not hispanic
sn1 = 0/1 indicator 1 main symptom
sn2 = 0/1 indicator 2 main symptoms
sn3 = 0/1 indicator 3 main symptoms 
all4 = 0/1 indicator all 4 main symptoms
--->
```{r include = F, warning= F, message=F}
#load libraries and data

library(survival)
library(survminer)
library(dplyr)
library(cluster)
library(randomForest)
library(ResourceSelection)

kelly <- read.table("kellydat.txt", header = T)
kelly <- kelly %>% mutate(count_sn = sn1 + sn2*2 + sn3*3 + all4*4) %>% select(-c(sn1, sn2, sn3, all4))
```


```{r echo = F}
#functions
kern_distribution <- function(x,knots = 4) {
  ### s: #controls how wide the kernels are, s should be half the distance between two knots 
  ### tau: decides where the breaks are based on the number of knots
  ###knots: number of splits/peaks in the data
  ##x and y are the data
  
  tau <- seq(min(x), max(x), length.out = knots)
  s <- diff(tau)[1]/2
  X <- matrix(0, nrow=length(x),ncol=knots)
  for(i in 1:knots) {
    X[,i] <- x * dnorm(x, tau[i], s)
  }
  return(X)
}
```
#Overview: Model Explorations

Multiple models and analyses were explored to find the best model that fit the neurological assessment data. Methods explored included:

1. Cox Proportional Hazards Model

2. Kaplan-Meier Estimate

3. Random Forest

4. Kernel Regression


#Cox Proportional Hazards Model
A Cox Proportional Hazards model was fit with nctdel as the response, and the features male, black, hispanic, and coun_sn as a factor (Appendix A for summary).
```{r echo = F}
res.cox <- coxph(Surv(nctdel, fail) ~ male + black + hisp + factor(count_sn), data =  kelly)
```
Looking at the Cox Model, only one of the coefficients are significant in detecting the nctdel waiting time--if the number of symptoms = 4. Either the data is not informative or the model is not a good fit of the data.

To assess the goodness of fit of the model, we check to see if the model fits the Cox Proportional Hazards Model Assumptions of no influential outliers, linearity, and homoscedascity of Schoenfeld residuals. The dataset does not satisfy the linearity assumption, so this implies that the Cox Proportional Hazard Model may not be the best model for the dataset (Appendix B).    


##Kaplan-Meier Estimate
The Kaplan-Meier estimate can take into account the fail variable properly in the dataset. It is clear that sample size is an issue for some of categories but for the most part we can observe general relationships between categories in our dataset. It seems that females are treated slightly faster than males although not significantly. Non-blacks appear to be treated faster than blacks with a 95% CI for non-blacks between 1.33 and 1.62 while a 95% CI for blacks is between 1.68 and 2.08. This appears to be significant bias, however there appears to be no bias with regard to hispanics. Finally, by looking at the graph for the Kaplan-Meier estimate on the dataset based on count of symptoms, we observe that symptom count does appear to be a major factor in wait time, especially when all four symptoms are present (Appendix D). 

#Random Forest for Two Different Responses
We built a random forest tree to identify the most significant features. In doing so, we realized that changing the response could change the significance ranking of features. 

##nctdel: Wait Time
```{r echo = F, warning = F}
rf.randomForest <- randomForest(nctdel ~., kelly)
varImpPlot(rf.randomForest)
```

Based on the variable importance plot, the most significant variables for determining nctdel wait time are in the order: fail, hispanic, count of symptoms, sex, and black.


##Failure
```{r echo = F, warning= F}
rf.randomForest <- randomForest(fail ~., kelly)
varImpPlot(rf.randomForest)
```

However, when changing the response to failure (0/1), the most significant predictors are nctdel, count_sn, and male. Being hispanic is less important.

Originally, for the CPH model and KM model, we used nctdel as the response, and noticed goodness-of-fit issues. We decided to create a Kernel Regression model with failure as the response to explore the robustness of this new model.

##Kernel Regression with 13 Bins
13 bins were calculated to fit the kernels. The bins are unevenly spaced because the data has a higher concentration of points for the feature nctdel between 0 and 2, even though its range is (0,26.25). The model has fail as the response, a kernel estimation of nctdel with 4 knots, and the features male, black, hispanic, and coun_sn. The bin levels are (-Inf,0], (0,0.3], (0.3,0.7], (0.7,1], (1,1.1], (1.1,1.3], (1.3,1.6], (1.6,1.9], (1.9,10], (10,13], (13,15], (17, Inf]. A summary of the model noted some significance for the feature nctdel (for the full summary, Appendix C).
```{r echo = F}
# set up the kernels
#bin <- cut(kelly$nctdel, c(-Inf,seq(0,26,.3),Inf))
bin <- cut(kelly$nctdel, c(-Inf,0,0.3,0.7,1,1.1,seq(1.3,2.05,0.3),10,13,15,17,Inf))
#bins <- cut(kelly$nctdel, c(-Inf,0,0.5, 0.7, 1.3, 2.05,10,15,Inf)) #manually choose bins based on quantiles and my own knowledge
levels(bin) <- as.character(seq(1:nlevels(bin)))
kelly$bins = bin
# we're going to pre-calculate a bunch of kernel weights
# each row is one bin, the columns are the kernel weights for that bin
```

```{r echo = F, ignore = T}
#trial and error
kernel.weights <- as.matrix(kelly %>% group_by(bins) %>% summarise(mean(nctdel),mean(male), mean(black), mean(hisp), mean(count_sn)))

#take 2
attach(kelly)
kernel.weights_2 <- data.frame(cbind(bin,kern_distribution(log(nctdel+1)), (kern_distribution(male)), (kern_distribution(black)), (kern_distribution(hisp))))

kernel.weights <- kernel.weights_2 %>% group_by(bin) %>% summarise(mean(V2), mean(V3), mean(V4), mean(V5), mean(V6), mean(V7), mean(V8), mean(V9), mean(V10), mean(V11), mean(V12), mean(V13), mean(V14), mean(V15),mean(V16), mean(V17))

detach(kelly)
```


```{r echo = F}
#take 3
attach(kelly)
kernel.weights_3 <- data.frame(cbind(bin,kern_distribution(log(nctdel+1)), male, black, hisp))
kernel.weights <- kernel.weights_3 %>% group_by(bin) %>% summarise(mean(V2), mean(V3), mean(V4), mean(V5), mean(male), mean(black), mean(hisp), mean(count_sn))

colnames(kernel.weights) <- c("bin","X1","X2", "X3", "X4", "male", "black", "hisp", "count_sn")

# now we'll create a new transform function
kernel.transformation <- function(time, censored) {
  y <- rep(0,time)
  if(!censored) {
    y[time] <- 1
  }
  
  # X has kernel weights instead of the bin indicators that it had before
  X <- (kernel.weights[1:time,-1])#,dim=c(time,ncol(kernel.weights)-1)) ###how to translate this line??
  
  return(data.frame(y,X))
}

d2 <- kernel.transformation((as.numeric(kelly$bins)[1]), as.logical(kelly$fail[1]))

d2_full <- NULL
for(i in 1:length(kelly$nctdel)) {
  d2_full <- rbind(d2_full,kernel.transformation(as.numeric(bins)[i], as.logical(kelly$fail[i])))
}
```

```{r echo = F, warning=F}
# fit a model

m2 <- glm(y ~ 0 + ., data=data.frame(d2_full), family="binomial")
#summary(m2)

# make a smooth plot
```

```{r echo = F}
X.pred <- kernel.weights[,-1]

hazard.logodds <- predict(m2, newdata = data.frame(X.pred), type = "response")

hazard <- exp(hazard.logodds)/(1+exp(hazard.logodds))
plot(1:nlevels(bin), hazard, type='l')

# you can choose different numbers of kernels or even use un-evenly spaced kernels to capture more detail early on where you have more data.
```
Looking at the hazard plot, after bin 6 (1.1,1.3], the survival log odds are significantly lower.


##Model Checks
To understand how well the model fits, we performed goodness of fit tests and a model check assumptions.
```{r echo = F}
par(mfrow = c(2,2))
plot(m2)
```
Looking at the Residuals vs. Fitted Graph, the points do not exhibit a pattern. However, the points are not evenly distrbuted, so they are heteroscedastic. Similarly, with Residuals vs. Leverage, the points are not homoscedastically distributed. While this model better fits the data than previous models, this model can also be improved. 
```{r echo = F}
X.test <- kelly %>% select(male, black, hisp, count_sn)

X.test <- data.frame(cbind(kern_distribution(kelly$nctdel), X.test))



kern.pred <- round(predict.glm(m2, newdata = data.frame(X.test), type = 'response'))
m = mean(kern.pred != kelly$fail)
```
Overall, the model fits the true dataset `r m*100`% of the time when predicting for failure over the entire dataset, which reaffirms the fact that the model can be improved. 

#Conclusion
Overall, most of the models explored were average at best. The best model for the dataset currently is the Kernel Regression Model. For next week, more model exploration and testing will be conducted to improve the model fit.

#Appendix

##A
###CPH Model Summary
```{r echo = F}
summary(res.cox)
```

##B

###Schoenfeld Residuals
```{r echo = F}
fit.cox <- cox.zph(res.cox)
ggcoxzph(fit.cox)
```
From the graphical inspection, there exists a pattern (slight curve in tails) with time for the feature black. The assumption of proportional hazards appears to be supported for the covariates male, each factor of the symptoms, and hispanic.

###Test for Outliers
```{r echo = F}
ggcoxdiagnostics(res.cox, type = "dfbeta",
                 linear.predictions = FALSE, ggtheme = theme_bw())
```
Most of the plots show no trends, so no points are significantly influential.

###Linearity
```{r echo = F}
ggcoxfunctional(Surv(nctdel, fail) ~ count_sn+1 + log(count_sn+1) + sqrt(count_sn+1), data = kelly)
```
The feature count_sn does not follow a linear trend, so it breaks the linearity assumption for a CPH Model.


##C

###Kernel Regression Summary
```{r echo = F}
summary(m2)
```

#D

###Kaplan Meier Estimate for SubCategories
The following is the Kaplan-Meier estimate for all subcategories present in the data set. Larger sample sizes would be useful.
```{r warning = F, echo = F}
d2 <- Surv(kelly$nctdel, kelly$fail) #creates a survival object #preprocessed version of the data

# km for male/female, red=female, blue=male
kmM <- survfit(d2 ~ kelly$male)
plot(kmM, col = c("red","blue"))
print(kmM)

# km for black/non-black, red=nonblack, blue=black
kmB <- survfit(d2 ~ kelly$black)
plot(kmB, col = c("red","blue"))
print(kmB)

# km for hispanic/non-hispanic, red=nonhisp, blue=hisp
kmH <- survfit(d2 ~ kelly$hisp)
plot(kmH, col = c("red","blue"))
print(kmH)

# KM for count of symptoms, red=0 to blue=5
kmCOS <- survfit(d2 ~ kelly$count_sn)
plot(kmCOS, col = c("red","orange","yellow","green","blue"))
print(kmCOS)

km <- survfit(d2 ~  kelly$male + kelly$black + kelly$hisp + kelly$count_sn)
print(km)
```

#Contributions
Grant:
Ian: Kaplan Meier
Sonia: Everything else